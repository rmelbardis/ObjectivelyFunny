{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7be37490",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import ImageColorGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2a606d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2e2abc",
   "metadata": {},
   "source": [
    "# Preprocessing of Text for Analysis Purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681b5afa",
   "metadata": {},
   "source": [
    "## Import the new dataframe\n",
    "(that has artist names removed from transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8523c0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('../raw_data/df_all_clean .json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0a32f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "555"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f95465",
   "metadata": {},
   "source": [
    "## Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d62d672",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Specific functions to our data - Scraps from the Loft and BBC\n",
    "\n",
    "# remove notes (Bo Burnham Only)\n",
    "def clean_bo(text):\n",
    "    txt = text\n",
    "    for note in '♫♪':\n",
    "        txt = txt.replace(note, '')\n",
    "    return txt\n",
    "\n",
    "def remove_music(text):\n",
    "    text = re.sub('♪.*?♪', '', text) # remove ♪ stuff that looks like this ♪\n",
    "    text = re.sub('♫.*?♫', '', text) # remove ♫ stuff that looks like this ♫\n",
    "    return text\n",
    "\n",
    "def remove_bracketed(text):\n",
    "    text = re.sub('\\[.*?\\]', '', text) # remove [stuff that looks like this]\n",
    "    text = re.sub('\\(.*?\\)', '', text) # remove (stuff that looks like this)\n",
    "    return text\n",
    "\n",
    "def remove_speaker_tags(text):\n",
    "    text = re.sub('\\s[\\w-]+( \\w+)?:\\s', ' ', text) # remove Word: or Word word: with a newline or space before\n",
    "    return text\n",
    "\n",
    "def remove_info(text):\n",
    "    text = re.sub('subtitle(s)? by .*', '', str(text)) # remove subtile(s) by xxxx\n",
    "    text = re.sub('(a)? netflix (original )?(comedy )?(special ?)?', '', text) # remove A Netflix Original Comedy Special\n",
    "    text = re.sub('(this )?(programme )?(contains )?(very |some )?strong language( |\\.)', '', text) # remove strong language\n",
    "    text = re.sub('adult humou?r( |\\.?)?', '', text) # remove adult humour\n",
    "    text = re.sub('(original )?air date', '', text) # remove air date\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0117c700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general functions for text pre-processing\n",
    "def remove_punc(text, chars):\n",
    "    txt = text\n",
    "    for punc in chars:\n",
    "        txt = txt.replace(punc, '')\n",
    "    return txt\n",
    "\n",
    "def remove_num(text):\n",
    "    return ''.join(char for char in text if not char.isdigit())\n",
    "\n",
    "def remove_stopw(text, word_list):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    return ' '.join(w for w in word_tokens if not w in word_list)\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ' '.join(lemmatizer.lemmatize(word) for word in text.split(' ') if len(lemmatizer.lemmatize(word))>2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1bfa464",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer_dict = {'got': 'get',\n",
    "                  'gon': 'go',\n",
    "                  'said': 'say',\n",
    "                   'saying': 'say',\n",
    "                  'fucking': 'fuck',\n",
    "                  'went': 'go',\n",
    "                  'finding': 'find',\n",
    "                  'getting': 'get'}\n",
    "\n",
    "def manual_lemmatizer(text):\n",
    "    for k, v in lemmatizer_dict.items():\n",
    "        text = text.replace(k, v)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f3e000",
   "metadata": {},
   "source": [
    "## Modifying & applying removal lists "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98b250e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a81d93",
   "metadata": {},
   "source": [
    "### Remove everything in Brackets, Music notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c90eedb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/jf61hd8n2cj411r4h4mdpyf00000gn/T/ipykernel_88261/137205631.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_df['full_transcript_clean'][clean_df['artist']=='Bo Burnham'] = clean_df[\n"
     ]
    }
   ],
   "source": [
    "clean_df = df.copy()\n",
    "\n",
    "## Step 1: Remove everything in Brackets, Music notes\n",
    "clean_df['full_transcript_clean'] = clean_df['full_transcript'].apply(remove_bracketed)\n",
    "\n",
    "### clean Bo before removing music\n",
    "clean_df['full_transcript_clean'][clean_df['artist']=='Bo Burnham'] = clean_df[\n",
    "    'full_transcript_clean'][clean_df['artist']=='Bo Burnham'].apply(clean_bo)\n",
    "\n",
    "clean_df['full_transcript_clean'] = clean_df['full_transcript_clean'].apply(remove_music)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0467722c",
   "metadata": {},
   "source": [
    "### Lowercase, remove useless regex matches\n",
    "Including specific scraps/BBC format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd8c7d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase all the words\n",
    "clean_df['full_transcript_clean'] = clean_df['full_transcript_clean'].str.lower()\n",
    "\n",
    "# remove speaker tags and info regexes\n",
    "clean_df['full_transcript_clean'] = clean_df['full_transcript_clean'].apply(remove_info)\n",
    "clean_df['full_transcript_clean'] = clean_df['full_transcript_clean'].apply(remove_speaker_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae58a0e9",
   "metadata": {},
   "source": [
    "### Remove numbers and stopwords + common comedy words, remove punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "010cba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['full_transcript_clean'] = clean_df['full_transcript_clean'].apply(remove_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39740276",
   "metadata": {},
   "outputs": [],
   "source": [
    "### additional words to remove from the scripts\n",
    "words_to_remove = ['thank', 'cheering', 'recorded', 'applause', 'laughter', 'laughing', 'murmuring', 'chatter',\n",
    "                       'aired', 'filmed', 'ladies', 'gentlemen', 'welcome', 'stage', 'transcript', 'netflix',\n",
    "                  'apollo', 'like', 'goodnight', 'mutter', 'noo', 'nuh', 'oof', 'maan', 'fuck', 'cause', 'okay', \n",
    "                   'hey', 'also', 'someone', 'somebody', 'everybody', 'also', 'part' , 'sometimes', 'maybe', \n",
    "                   'three', 'second', 'everything', 'minute', 'name', 'kind', 'point', 'yeah', 'hello', 'one', \n",
    "                   'two', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n",
    "                    \n",
    "                    # 'know'? 'go'? 'fuck'?\n",
    "    \n",
    "                    # haven't left 'i'm' etc. as those should be cleaned up\n",
    "                    # by a mixture of stopwords, punctuation removeal, lemmatizing and minimum length\n",
    "\n",
    "stopwords_plus = words_to_remove + stopwords.words('english')\n",
    "\n",
    "clean_df['full_transcript_clean'] = clean_df['full_transcript_clean'].apply(remove_stopw, args=(stopwords_plus,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5da635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "punc_added = string.punctuation + '“”‘’…♪♫¶'\n",
    "\n",
    "clean_df['full_transcript_clean'] = clean_df['full_transcript_clean'].apply(remove_punc, args=(punc_added,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008b37a1",
   "metadata": {},
   "source": [
    "### Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2b73acf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_df['full_transcript_clean'] = clean_df['full_transcript_clean'].apply(lemmatize).apply(manual_lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbd23189",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_transcript</th>\n",
       "      <th>artist</th>\n",
       "      <th>show_name</th>\n",
       "      <th>year</th>\n",
       "      <th>source</th>\n",
       "      <th>full_transcript_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[rock music playing]\\n\\n[indistinct chatter]\\n...</td>\n",
       "      <td>Adam Devine</td>\n",
       "      <td>Best Time Of Our Lives</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>Scraps from the Loft</td>\n",
       "      <td>man let right guy much take seat guy get jacke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>strong language. CHEERING Yes, yes, yes! How...</td>\n",
       "      <td>Adam Hess</td>\n",
       "      <td>Live from the BBC</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>BBC</td>\n",
       "      <td>yes yes yes well lovely going apologise state ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ladies and gentlemen, please welcome your hos...</td>\n",
       "      <td>Adam Hills</td>\n",
       "      <td>Live at the Apollo Series 9 Episode 4</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>BBC</td>\n",
       "      <td>please host tonight london london live amazing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>some strong language and adult humour Ladies...</td>\n",
       "      <td>Adam Hills</td>\n",
       "      <td>Live at the Apollo Series 12 Episode 4</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>BBC</td>\n",
       "      <td>please host tonight know hair lost bet british...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>? CHEERING Hello, Apollo. I am going to start ...</td>\n",
       "      <td>Adam Hills</td>\n",
       "      <td>Live at the Apollo Series 5 Episode 5</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>BBC</td>\n",
       "      <td>going start say something probably never heard...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     full_transcript       artist  \\\n",
       "0  [rock music playing]\\n\\n[indistinct chatter]\\n...  Adam Devine   \n",
       "1    strong language. CHEERING Yes, yes, yes! How...    Adam Hess   \n",
       "2   Ladies and gentlemen, please welcome your hos...   Adam Hills   \n",
       "3    some strong language and adult humour Ladies...   Adam Hills   \n",
       "4  ? CHEERING Hello, Apollo. I am going to start ...   Adam Hills   \n",
       "\n",
       "                                show_name    year                source  \\\n",
       "0                  Best Time Of Our Lives  2019.0  Scraps from the Loft   \n",
       "1                       Live from the BBC  2016.0                   BBC   \n",
       "2   Live at the Apollo Series 9 Episode 4  2013.0                   BBC   \n",
       "3  Live at the Apollo Series 12 Episode 4  2016.0                   BBC   \n",
       "4   Live at the Apollo Series 5 Episode 5  2009.0                   BBC   \n",
       "\n",
       "                               full_transcript_clean  \n",
       "0  man let right guy much take seat guy get jacke...  \n",
       "1  yes yes yes well lovely going apologise state ...  \n",
       "2  please host tonight london london live amazing...  \n",
       "3  please host tonight know hair lost bet british...  \n",
       "4  going start say something probably never heard...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61182d21",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## LDA model to see topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81eb26f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "new_sw = ['whine', 'hnn', 'malla', 'letta', 'namoo', 'getta', 'nama', 'mana', 'chk',\n",
    "         'manoo', 'hadda', 'ama', 'carlin']\n",
    "clean_df['full_transcript_clean'] = clean_df['full_transcript_clean'].apply(remove_stopw, args=(new_sw,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ea437f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer().fit(clean_df['full_transcript_clean'])\n",
    "data_vectorized = vectorizer.transform(clean_df['full_transcript_clean'])\n",
    "lda_model = LatentDirichletAllocation(n_components=2).fit(data_vectorized)\n",
    "\n",
    "def print_topics(model, vectorizer):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print([(vectorizer.get_feature_names()[i], topic[i])\n",
    "                        for i in topic.argsort()[:-10 - 1:-1]])\n",
    "        \n",
    "\n",
    "print_topics(lda_model, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a75afc",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edb9e3f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "token_df = clean_df['full_transcript_clean'].apply(tokenize).astype(str)\n",
    "token_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95afd28a",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.Series([y for x in token_df.values.flatten() for y in x.split()]).value_counts().head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6da549",
   "metadata": {},
   "source": [
    "## Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae1d594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_bow(df):\n",
    "    keys = df.sum().index\n",
    "    values = df.sum().values\n",
    "    bow_dict = dict(zip(keys, values))\n",
    "    return bow_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8802a00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first bow\n",
    "\n",
    "t = clean_df['full_transcript_clean'].astype(str)\n",
    "\n",
    "vectorizer = CountVectorizer(min_df = 0.3, max_df = 0.8, ngram_range=(1,1))\n",
    "X = vectorizer.fit_transform(t)\n",
    "bow_df1 = pd.DataFrame(X.toarray(),columns = vectorizer.get_feature_names())\n",
    "len(bow_df1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e85354",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_1 = dict_bow(bow_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcca773a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second bow\n",
    "\n",
    "t = clean_df['full_transcript_clean'].astype(str)\n",
    "\n",
    "vectorizer = CountVectorizer(min_df = 0.4, max_df = 0.8, ngram_range=(1,1))\n",
    "X = vectorizer.fit_transform(t)\n",
    "bow_df2 = pd.DataFrame(X.toarray(),columns = vectorizer.get_feature_names())\n",
    "\n",
    "dict_2 = dict_bow(bow_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4959ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# third bow\n",
    "\n",
    "t = clean_df['full_transcript_clean'].astype(str)\n",
    "\n",
    "vectorizer = CountVectorizer(min_df = 0.4, max_df = 0.8, max_features=50, ngram_range=(1,2))\n",
    "X = vectorizer.fit_transform(t)\n",
    "bow_df3 = pd.DataFrame(X.toarray(),columns = vectorizer.get_feature_names())\n",
    "\n",
    "dict_3 = dict_bow(bow_df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cf3172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fourth bow\n",
    "\n",
    "t = clean_df['full_transcript_clean'].astype(str)\n",
    "\n",
    "vectorizer = CountVectorizer(min_df = 0.4, max_df = 0.7, max_features=50, ngram_range=(1,2))\n",
    "X = vectorizer.fit_transform(t)\n",
    "dict_4 = dict_bow(pd.DataFrame(X.toarray(),columns = vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e48e137",
   "metadata": {},
   "source": [
    "## Word cloud with frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be78a019",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_mask = np.array(Image.open('../raw_data/emoji.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c61fb4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_word_cloud(wc, bow_dict):\n",
    "    word_cloud = wc.generate_from_frequencies(bow_dict)\n",
    "    return word_cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "605892ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot word cloud\n",
    "def plot_cloud(wordcloud, mask):\n",
    "    # Set figure size\n",
    "    plt.figure(figsize=(30, 20))\n",
    "    # Display image\n",
    "    image_colors = ImageColorGenerator(mask)\n",
    "    plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\") \n",
    "    # No axis details\n",
    "    plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68944568",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(width=3000, height = 2000, background_color='white',\n",
    "                       random_state=1, collocations=False, stopwords = STOPWORDS, mask=emoji_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5595f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_bow(df):\n",
    "    keys = df.sum().index\n",
    "    values = df.sum().values\n",
    "    bow_dict = dict(zip(keys, values))\n",
    "    return bow_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ee77e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_some_cloud(index, mask):\n",
    "    t = clean_df['full_transcript_clean'][index]\n",
    "    if type(t)!=str:\n",
    "        t = t.astype(str)\n",
    "        vectorizer = CountVectorizer(min_df = 0.4, max_df = 0.8, ngram_range=(1,2), max_features=80)\n",
    "    else:\n",
    "        vectorizer = CountVectorizer(max_features=80)\n",
    "    X = vectorizer.fit_transform(t)\n",
    "    temp_dict = dict_bow(pd.DataFrame(X.toarray(),columns = vectorizer.get_feature_names_out()))\n",
    "    word_cloud = make_word_cloud(wc, dict_5)\n",
    "    plot_cloud(word_cloud, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8d9298",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_some_cloud(emoji_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335e559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fifith bow\n",
    "\n",
    "temp_series.clean_df['full_transcript_clean'].copy()\n",
    "t = temp_series.astype(str)\n",
    "\n",
    "vectorizer = CountVectorizer(min_df = 0.4, max_df = 0.8, ngram_range=(1,2), max_features=80)\n",
    "\n",
    "X = vectorizer.fit_transform(t)\n",
    "dict_5 = dict_bow(pd.DataFrame(X.toarray(),columns = vectorizer.get_feature_names_out()))\n",
    "\n",
    "word_cloud = make_word_cloud(wc, dict_5)\n",
    "plot_cloud(word_cloud, emoji_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb48008b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df['artist'] == 'George Carlin'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e872db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['artist'] == 'Mae Martin'].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8b1551",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_series.iloc[360]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e20198",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.Series(temp_series.iloc[360])\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2), max_features=40)\n",
    "\n",
    "X = vectorizer.fit_transform(t)\n",
    "dict_6 = dict_bow(pd.DataFrame(X.toarray(),columns = vectorizer.get_feature_names_out()))\n",
    "\n",
    "word_cloud = make_word_cloud(wc, dict_6)\n",
    "plot_cloud(word_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95d82db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EI = temp_series.iloc[163:168]\n",
    "t = EI.astype(str)\n",
    "\n",
    "vectorizer = CountVectorizer(min_df = 0.4, max_df = 0.8, ngram_range=(1,2), max_features=80)\n",
    "\n",
    "X = vectorizer.fit_transform(t)\n",
    "dict_6 = dict_bow(pd.DataFrame(X.toarray(),columns = vectorizer.get_feature_names_out()))\n",
    "\n",
    "word_cloud = make_word_cloud(wc, dict_6)\n",
    "plot_cloud(word_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23988b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "carlin_script = temp_series.iloc[190:210]\n",
    "\n",
    "t = carlin_script.astype(str)\n",
    "\n",
    "vectorizer = CountVectorizer(min_df = 0.4, max_df = 0.8, ngram_range=(1,2), max_features=80)\n",
    "\n",
    "X = vectorizer.fit_transform(t)\n",
    "dict_6 = dict_bow(pd.DataFrame(X.toarray(),columns = vectorizer.get_feature_names_out()))\n",
    "\n",
    "word_cloud = make_word_cloud(wc, dict_6)\n",
    "plot_cloud(word_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a5fe60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb1e2171",
   "metadata": {},
   "source": [
    "# ----- **note to self: start from here**-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79560189",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_transcripts = ' '.join(clean_df['full_transcript_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dd46d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(full_transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411c03af",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud = WordCloud(width=3000, height = 2000,\n",
    "                       random_state=1, colormap='Pastel1',\n",
    "                       collocations=False, stopwords = STOPWORDS).generate(full_transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8b9c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cloud(word_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26310b19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
