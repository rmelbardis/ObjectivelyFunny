{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7914c7a2",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Le Wagon Data Science Project Pitch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9420eb0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Objectively Funny\n",
    "### Adventures in Comedy AI\n",
    "Reinis Melbardis\n",
    "25/02/2022, London"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d36db13",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"https://miro.medium.com/max/750/1*MzKEdDDJhdrmRP6ssOuleQ.jpeg\" \n",
    "     alt = \"C3PO being a jokester\"\n",
    "     width=\"300\" \n",
    "     height=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b607bf1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c0c3f3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "üòÉ Comedy is nuanced and a highly \"human\" thing.\n",
    "\n",
    "üìö Can we gain insight into what humour is through data analysis?\n",
    "\n",
    "üß† Can artificial brains be funny?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278ea887",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## üìú Data\n",
    "### Full list\n",
    "\n",
    "[Scraps From the Loft - Full Scripts of Stand-up specials](http://scrapsfromtheloft.com/stand-up-comedy-scripts/) - ‚ùó\n",
    "\n",
    "[IMDB - Review Scores](https://www.imdb.com/)\n",
    "\n",
    "[Rotten Tomatoes - Review Scores](https://www.rottentomatoes.com/)\n",
    "\n",
    "[Kaggle SComedy](https://www.kaggle.com/eduardo4jesus/scomedy)\n",
    "\n",
    "[Kaggle Short Jokes](https://www.kaggle.com/abhinavmoudgil95/short-jokes)\n",
    "\n",
    "Netflix Subtitles (if more needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3249274e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def scrape_loft():\n",
    "\n",
    "    '''\n",
    "    Scrapes every title and url from the list of stand-up comedy scripts on Scraps From the Loft\n",
    "    returns a dictionary of urls and titles \n",
    "    '''\n",
    "    url = 'http://scrapsfromtheloft.com/stand-up-comedy-scripts/'\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    titles_html = soup.find_all(['h1', 'h3'], class_=\"elementor-post__title\")\n",
    "    \n",
    "    title_list = []\n",
    "    url_list = []\n",
    "    for title in titles_html:\n",
    "        script_url = title.find('a').get('href')\n",
    "        script_title = title.find('a').string.strip('\\n').strip('\\t')\n",
    "        url_list.append(script_url)\n",
    "        title_list.append(script_title)\n",
    "    \n",
    "    return {'url': url_list, 'title': title_list}\n",
    "\n",
    "def scrape_script(url):\n",
    "    '''\n",
    "    Scrapes descriptions and full scripts of individual pages from stand-up comedy scripts on Scraps From the Loft\n",
    "    returns: description_list, script_list\n",
    "    '''\n",
    "    script_response = requests.get(url)\n",
    "    script_soup = BeautifulSoup(script_response.content, 'html.parser')\n",
    "    \n",
    "    description = script_soup.find(name='div', attrs={'data-id': \"53e7c39\"})\\\n",
    "                .find('div', class_=\"elementor-widget-container\").string.strip('\\n').strip('\\t')\n",
    "    \n",
    "    content = script_soup.find_all('p', attrs={'style': \"text-align: justify;\"})\n",
    "    \n",
    "    _lst = []\n",
    "    for paragraph in content:\n",
    "        text = ''.join(list(paragraph.strings))\n",
    "        _lst.append(text)\n",
    "    full_script = '\\n\\n'.join(_lst)\n",
    "    \n",
    "    return [description, full_script]\n",
    "\n",
    "def script_dict(url_list):\n",
    "    descriptions = []\n",
    "    full_transcripts = []\n",
    "    for url in url_list:\n",
    "        _result = scrape_script(url)\n",
    "        descriptions.append(_result[0])\n",
    "        full_transcripts.append(_result[1])\n",
    "    return {'description': descriptions, 'full_transcript': full_transcripts}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c43138a8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scrape_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43mscrape_dict\u001b[49m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mscripts}\n\u001b[1;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(df_dict)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scrape_dict' is not defined"
     ]
    }
   ],
   "source": [
    "df_dict = {**scrape_dict, **scripts}\n",
    "df = pd.DataFrame(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed5a73e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import string \n",
    "\n",
    "def remove_punc(text):\n",
    "    txt = text\n",
    "    for punc in string.punctuation:\n",
    "        txt = txt.replace(punc, '')\n",
    "    return txt\n",
    "\n",
    "df['full_words'] = df['full_transcript'].apply(remove_punc).apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34a3fed",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "df['num_words'] = df['full_words'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6464dc37",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "df['num_words'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84de575",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Main Source - Scraps From the Loft\n",
    "- 384 individual scripts (not all usable)\n",
    "- full-length scripts have ~10 000 words per script\n",
    "- Expect approximately **3.5m words** in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f271dc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68d5c5cc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull_transcript\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m3\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "print(df['full_transcript'].iloc[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a29986",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428f2094",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "üéØ Focus on two things:\n",
    "- Analysis of stand-up comedy and written jokes\n",
    "- Training an AI to write stand-up, preferably with hilarious consequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb46f10",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ‚úçÔ∏è Analysis of Words - Machine Learning NLP and Deep Learning NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b0f32c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- [Scraps from the Loft](http://scrapsfromtheloft.com/stand-up-comedy-scripts/) dataset of comedy special scripts (possibly also Netflix subtitles if more is needed)\n",
    "\n",
    "- Cross reference against [IMDB](https://www.imdb.com/)/[Rotten Tomatoes](https://www.rottentomatoes.com/) scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6dce6b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ea0abb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Analysis:\n",
    "    - sequences & word use per special - how do comedians differ?\n",
    "    - NLP ML and DL on review scores - can we quantify what is ‚Äúgood comedy‚Äù from words alone?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cab8def",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937dd399",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ü§ñ Create a Comedy Bot using Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836d79e8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Process**\n",
    "1. Use the same dataset & potentially some other clean data from Kaggle\n",
    "\n",
    "2. Use a Deep Learning model (pre-trained or new) to train a Comedy Bot on the dataset(s) assembled\n",
    "\n",
    "3. Ideally provide the bot with 1-2 word topics, or a particular comedian's style to create stand-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af93de4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers\n",
    "\n",
    "#------------------------------------------------#\n",
    "# Generative Adversarial Network Encoder-Decoder #\n",
    "#------------------------------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70418880",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Comments**\n",
    "\n",
    "Not expecting it to be good, but likely going to be unfunny in a funny way.\n",
    "\n",
    "Really want this to happen, but also likely to be difficult, involve research papers and things we haven't done on the course.\n",
    "\n",
    "Happy to be flexible with scope as we go along."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a50bd1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### üåç Create an Interactive Website to present on Demo Day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce95093",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Analysis/NLP:** Drop-down menus for different comedians, classifying their style, word-clouds, topics\n",
    "- **Comedy Bot:**\n",
    " - Best stand-up it has mustered\n",
    " - Dropdowns of different styles/comedians and a prompt to generate new content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006e80fc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Optional:** awful 90's-style HTML\n",
    "\n",
    "<img src=\"https://assets-global.website-files.com/6009ec8cda7f305645c9d91b/61ddb9ed55fdd4017838353b_zPX4UBBjtWeemWC2Ts6yDMxC26OmeytCnjFvCf57lHcwOVhingf-rZX5WwwU7NCBoq7ILa12-zB9ae5_gHMdy3C3t5iFAZvvcTe4aOzsnM4zfezw00efjoiTE_XwcKoBbyf6eClk.jpeg\" \n",
    "     alt = \"That's the good stuff!\"\n",
    "     width=\"800\" \n",
    "     height=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9706e542",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### üöÄ Stretch Goal - Audio Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40195202",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<img src=\"https://rogueamoeba.com/global/images/icons/legacy/96/soundflower.png\" \n",
    "     alt = \"Soundflower\"\n",
    "     width=\"50\" \n",
    "     height=\"50\" />\n",
    "\n",
    "Step 1: Stream Netflix comedy specials & capture sound using Soundflower (Mac only) or JACK Audio & capture subtitles directly from Netflix .srt files. Expect 50-100 routines of ~50-100hrs total.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01345ecd",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<img src=\"https://www.audacityteam.org/wp-content/themes/wp_audacity/img/logo.png\" \n",
    "     alt = \"Audacity\"\n",
    "     width=\"50\" \n",
    "     height=\"50\" />\n",
    "\n",
    "Step 2: use Audacity to process files into appropriate format.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe95bb5",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Step 3: Detect Laughter.\n",
    "\n",
    "[jrgillick Laughter Detection Repo](https://github.com/jrgillick/laughter-detection)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41645ff",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Step 4: Incorporate insights into analysis and Comedy Bot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07107d5a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Thank you for listening! I'm here all week!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
