{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c046cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2e2abc",
   "metadata": {},
   "source": [
    "# Preprocessing of Text for Analysis Purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681b5afa",
   "metadata": {},
   "source": [
    "## Import filtered dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8523c0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json('../raw_data/scraps_clean_new.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a606d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a32f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9773df07",
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in df2['name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e809bf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a21c467",
   "metadata": {},
   "outputs": [],
   "source": [
    "[a for a in df['artist'].unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46211904",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[285]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f95465",
   "metadata": {},
   "source": [
    "## Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be37490",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "import string\n",
    "import re \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d62d672",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Specific functions to standup/scraps from the loft\n",
    "def remove_music(text):\n",
    "    text = re.sub('♪.*?♪', '', text) # remove ♪ stuff that looks like this ♪\n",
    "    return text\n",
    "\n",
    "def remove_bracketed(text):\n",
    "    text = re.sub('\\[.*?\\]', '', text) # remove [stuff that looks like this]\n",
    "    text = re.sub('\\(.*?\\)', '', text) # remove (stuff that looks like this)\n",
    "    return text\n",
    "\n",
    "def remove_useless(text):\n",
    "    text = re.sub('\\n\\w+\\(\\s\\w+\\)?\\:\\s', '', text) # remove Word: or Word word: with a newline before\n",
    "    text = re.sub('subtitles? by \\w+', '', text) # remove subtile(s) by xxxx\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0117c700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general functions for text pre-processing\n",
    "def remove_punc(text, chars):\n",
    "    txt = text\n",
    "    for punc in chars:\n",
    "        txt = txt.replace(punc, '')\n",
    "    return txt\n",
    "\n",
    "def remove_num(text):\n",
    "    return ''.join(char for char in text if not char.isdigit())\n",
    "\n",
    "def remove_stopw(text, word_list):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    return ' '.join(w for w in word_tokens if not w in word_list)\n",
    "\n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ' '.join(lemmatizer.lemmatize(word) for word in text.split(' ') if len(lemmatizer.lemmatize(word))>2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b9f39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_lemmatizer(text):\n",
    "    text = text.replace('got', 'get').replace(\n",
    "        'gon', 'go').replace(\n",
    "        'said', 'say').replace(\n",
    "        'fucking', 'fuck').replace(\n",
    "        'went', 'go').replace(\n",
    "    'finding', 'find')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece4f7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f3e000",
   "metadata": {},
   "source": [
    "## Modifying & applying removal lists "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b250e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a81d93",
   "metadata": {},
   "source": [
    "### Remove everything in Brackets, Music notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90eedb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['full_transcript_clean'] = clean_df['full_transcript'].apply(remove_bracketed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898f0d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Remove ♪ from specific comedians:\n",
    "# [Bo Burnham]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83feb6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_df['full_transcript_clean'] = clean_df['full_transcript'].apply(remove_music)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0467722c",
   "metadata": {},
   "source": [
    "### Lowercase, remove useless regex matches, numbers, stopwords and punctuation\n",
    "Including specific scraps format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8c7d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175eaf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['full_transcript_clean'] = clean_df['full_transcript_clean'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf27c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['full_transcript_clean'] = clean_df['full_transcript_clean'].apply(remove_useless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39740276",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_remove = ['thank', 'cheering', 'recorded', 'applause', 'laughter', 'laughing', 'murmuring', 'chatter',\n",
    "                       'aired', 'filmed', 'ladies', 'gentlemen', \"that's\", \"i'm\", \"don't\"]\n",
    "# other possible removals 'netflix special', 'full transcript' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454482a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_plus = words_to_remove + stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889e9bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['full_transcript_clean'] = clean_df['full_transcript_clean'].apply(remove_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b357cc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['full_transcript_clean'] = clean_df['full_transcript_clean'].apply(remove_stopw, args=(stopwords_plus,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069293ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "punc_added = string.punctuation + '“”‘’…♪¶'\n",
    "\n",
    "clean_df['full_transcript_clean'] = clean_df['full_transcript_clean'].apply(remove_punc, args=(punc_added,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648cae8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['full_transcript_clean'] = clean_df['full_transcript_clean'].apply(lemmatize).apply(manual_lemmatizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008b37a1",
   "metadata": {},
   "source": [
    "### Remove numbers and stopwords + common comedy words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b73acf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e00c217",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df[clean_df.full_transcript.str.find('cazzo')!=-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec9afae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8802a00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = clean_df['full_transcript_clean'].astype(str)\n",
    "\n",
    "vectorizer = CountVectorizer(min_df = 0.3, max_df = 0.8, max_features = 20)\n",
    "X = vectorizer.fit_transform(t)\n",
    "bow_df = pd.DataFrame(X.toarray(),columns = vectorizer.get_feature_names())\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ea437f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "vectorizer = TfidfVectorizer().fit(clean_df['full_transcript_clean'])\n",
    "\n",
    "data_vectorized = vectorizer.transform(clean_df['full_transcript_clean'])\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=3).fit(data_vectorized)\n",
    "\n",
    "def print_topics(model, vectorizer):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print([(vectorizer.get_feature_names()[i], topic[i])\n",
    "                        for i in topic.argsort()[:-10 - 1:-1]])\n",
    "        \n",
    "\n",
    "print_topics(lda_model, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edb9e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_df = clean_df['full_transcript_clean'].apply(tokenize).astype(str)\n",
    "token_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95afd28a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.Series([y for x in token_df.values.flatten() for y in x.split()]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4868d8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequent_words = ['like']\n",
    "# clean_df['full_transcript_clean'] = clean_df['full_transcript_clean'].apply(remove_stopw, args=(frequent_words,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79560189",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_transcripts = ' '.join(clean_df['full_transcript_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dd46d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(full_transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605892ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot word cloud\n",
    "def plot_cloud(wordcloud):\n",
    "    # Set figure size\n",
    "    plt.figure(figsize=(30, 20))\n",
    "    # Display image\n",
    "    plt.imshow(wordcloud) \n",
    "    # No axis details\n",
    "    plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411c03af",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud = WordCloud(width=3000, height = 2000,\n",
    "                       random_state=1, colormap='Pastel1',\n",
    "                       collocations=False, stopwords = STOPWORDS).generate(full_transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8b9c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cloud(word_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c409c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
